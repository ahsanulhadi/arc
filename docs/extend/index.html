<!DOCTYPE html>
  
  
  
  
   <html class="no-js"> 

  <head lang="en-us">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=10" />
    <title>Extend - Arc</title>
    <meta name="generator" content="Hugo 0.51" />

    
    <meta name="description" content="Arc is an opinionated framework for defining data pipelines which are predictable, repeatable and manageable.">
    
    <link rel="canonical" href="https://arc.tripl.ai/extend/">
    
    <meta name="author" content="ai.tripl.arc">
    

    <meta property="og:url" content="https://arc.tripl.ai/extend/">
    <meta property="og:title" content="Arc">
    <meta property="og:image" content="https://arc.tripl.ai/images/logo.png">
    <meta name="apple-mobile-web-app-title" content="Arc">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <link rel="shortcut icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">
    <link rel="icon" type="image/x-icon" href="https://arc.tripl.ai/images/favicon.ico">

    <style>
      @font-face {
        font-family: 'Icon';
        src: url('https://arc.tripl.ai/fonts/icon.eot');
        src: url('https://arc.tripl.ai/fonts/icon.eot')
               format('embedded-opentype'),
             url('https://arc.tripl.ai/fonts/icon.woff')
               format('woff'),
             url('https://arc.tripl.ai/fonts/icon.ttf')
               format('truetype'),
             url('https://arc.tripl.ai/fonts/icon.svg')
               format('svg');
        font-weight: normal;
        font-style: normal;
      }
    </style>

    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/application.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/temporary.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/palettes.css">
    <link rel="stylesheet" href="https://arc.tripl.ai/stylesheets/highlight/highlight.css">

    
    
    
    <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ubuntu:400,700|Ubuntu&#43;Mono">
    <style>
      body, input {
        font-family: 'Ubuntu', Helvetica, Arial, sans-serif;
      }
      pre, code {
        font-family: 'Ubuntu Mono', 'Courier New', 'Courier', monospace;
      }
    </style>

    
    <script src="https://arc.tripl.ai/javascripts/modernizr.js"></script>

    

  </head>
  <body class="palette-primary-red palette-accent-teal">



	
	


<div class="backdrop">
	<div class="backdrop-paper"></div>
</div>

<input class="toggle" type="checkbox" id="toggle-drawer">
<input class="toggle" type="checkbox" id="toggle-search">
<label class="toggle-button overlay" for="toggle-drawer"></label>

<header class="header">
	<nav aria-label="Header">
  <div class="bar default">
    <div class="button button-menu" role="button" aria-label="Menu">
      <label class="toggle-button icon icon-menu" for="toggle-drawer">
        <span></span>
      </label>
    </div>
    <div class="stretch">
      <div class="title">
        Extend
      </div>
    </div>

    

    
    <div class="button button-github" role="button" aria-label="GitHub">
      <a href="https://github.com/tripl-ai/arc" title="@https://github.com/tripl-ai/arc on GitHub" target="_blank" class="toggle-button icon icon-github"></a>
    </div>
    
    
        
  </div>
  <div class="bar search">
    <div class="button button-close" role="button" aria-label="Close">
      <label class="toggle-button icon icon-back" for="toggle-search"></label>
    </div>
    <div class="stretch">
      <div class="field">
        <input class="query" type="text" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck>
      </div>
    </div>
    <div class="button button-reset" role="button" aria-label="Search">
      <button class="toggle-button icon icon-close" id="reset-search"></button>
    </div>
  </div>
</nav>
</header>

<main class="main">
	<div class="drawer">
		<nav aria-label="Navigation">
  <a href="https://arc.tripl.ai/" class="project">
    <div class="banner">
      
      <div class="logo">
        <img src="https://arc.tripl.ai/images/logo.png">
      </div>
      
      <div class="name">
        <strong>Arc 
          <span class="version">1.15.0</span></strong>
        
        <br> tripl-ai/arc 
      </div>
    </div>
  </a>

  <div class="scrollable">
    <div class="wrapper">
      

      <div class="toc">
        
        <ul>
          




<li>
  
    



<a  title="Tutorial" href="https://arc.tripl.ai/tutorial/">
	
	Tutorial
</a>



  
</li>



<li>
  
    



<a  title="Extract" href="https://arc.tripl.ai/extract/">
	
	Extract
</a>



  
</li>



<li>
  
    



<a  title="Transform" href="https://arc.tripl.ai/transform/">
	
	Transform
</a>



  
</li>



<li>
  
    



<a  title="Load" href="https://arc.tripl.ai/load/">
	
	Load
</a>



  
</li>



<li>
  
    



<a  title="Execute" href="https://arc.tripl.ai/execute/">
	
	Execute
</a>



  
</li>



<li>
  
    



<a  title="Validate" href="https://arc.tripl.ai/validate/">
	
	Validate
</a>



  
</li>



<li>
  
    



<a  title="Metadata" href="https://arc.tripl.ai/metadata/">
	
	Metadata
</a>



  
</li>



<li>
  
    



<a  title="Partials" href="https://arc.tripl.ai/partials/">
	
	Partials
</a>



  
</li>



<li>
  
    



<a  title="Patterns" href="https://arc.tripl.ai/patterns/">
	
	Patterns
</a>



  
</li>



<li>
  
    



<a  title="Deploy" href="https://arc.tripl.ai/deploy/">
	
	Deploy
</a>



  
</li>



<li>
  
    



<a class="current" title="Extend" href="https://arc.tripl.ai/extend/">
	
	Extend
</a>


<ul id="scrollspy">
</ul>


  
</li>



<li>
  
    



<a  title="Contributing" href="https://arc.tripl.ai/contributing/">
	
	Contributing
</a>



  
</li>



<li>
  
    



<a  title="License" href="https://arc.tripl.ai/license/">
	
	License
</a>



  
</li>


        </ul>
         
        <hr>
        <span class="section">The author</span>

        <ul>
           
          <li>
            <a href="https://github.com/tripl-ai" target="_blank" title="@tripl-ai on GitHub">
              @tripl-ai on GitHub
            </a>
          </li>
           
        </ul>
        
      </div>
    </div>
  </div>
</nav>
	</div>

	<article class="article">
		<div class="wrapper">
			<h1>Extend </h1>

			

<p>Arc can be exended in four ways by registering:</p>

<ul>
<li><a href="#dynamic-configuration-plugins">Dynamic Configuration Plugins</a>.</li>
<li><a href="#lifecycle-plugins">Lifecycle Plugins</a>.</li>
<li><a href="#pipeline-stage-plugins">Pipeline Stage Plugins</a>.</li>
<li><a href="#user-defined-functions">User Defined Functions</a> which extend the Spark SQL dialect.</li>
</ul>

<h2 id="dynamic-configuration-plugins">Dynamic Configuration Plugins</h2>

<h5 id="since-1-3-0">Since: 1.3.0</h5>

<div class="admonition note">
<p class="admonition-title">Dynamic vs Deterministic Configuration</p>
<p>Use of this functionality is discouraged as it goes against the <a href="https://arc.tripl.ai/#principles">principles of Arc</a> specifically around statelessness/deterministic behaviour but is inlcuded here for users who have not yet committed to a job orchestrator such as <a href="https://airflow.apache.org/">Apache Airflow</a> and have dynamic configuration requirements.</p>
</div>

<p>The <code>Dynamic Configuration Plugin</code> plugin allow users to inject custom configuration parameters which will be processed before resolving the job configuration file. The plugin must return a Java <code>Map[String, Object]</code> which will be included in the job configuration resolution step.</p>

<h3 id="examples">Examples</h3>

<p>For example a custom runtime configuration plugin could be used calculate a formatted list of dates to be used with an <a href="../extract">Extract</a> stage to read only a subset of documents:</p>

<pre><code class="language-scala">package ai.tripl.arc.plugins.config

import scala.collection.JavaConverters._

import ai.tripl.arc.plugins._
import ai.tripl.arc.util.log.logger.Logger

import java.sql.Date
import java.time.LocalDate
import java.time.format.{DateTimeFormatter, DateTimeFormatterBuilder}
import java.time.format.ResolverStyle

class DeltaPeriodDynamicConfigurationPlugin extends DynamicConfigurationPlugin {

  override def values(params: Map[String, String])(implicit logger: ai.tripl.arc.util.log.logger.Logger): java.util.Map[String, Object] = {
    val startTime = System.currentTimeMillis() 

    val stageDetail = new java.util.HashMap[String, Object]()
    stageDetail.put(&quot;type&quot;, &quot;DeltaPeriodDynamicConfigurationPlugin&quot;)
    stageDetail.put(&quot;pluginVersion&quot;, BuildInfo.version)
    stageDetail.put(&quot;params&quot;, params.asJava)

    logger.info()
      .field(&quot;event&quot;, &quot;enter&quot;)
      .map(&quot;stage&quot;, stageDetail)      
      .log()   

    // input validation
    val returnName = params.get(&quot;returnName&quot;) match {
        case Some(returnName) =&gt; returnName.trim
        case None =&gt; throw new Exception(&quot;required parameter 'returnName' not found.&quot;)
    }

    val lagDays = params.get(&quot;lagDays&quot;) match {
        case Some(lagDays) =&gt; {
            try {
            lagDays.toInt * -1
            } catch {
                case e: Exception =&gt; throw new Exception(s&quot;cannot convert lagDays ('${lagDays}') to integer.&quot;)
            }
        }
        case None =&gt; throw new Exception(&quot;required parameter 'lagDays' not found.&quot;)
    }

    val leadDays = params.get(&quot;leadDays&quot;) match {
        case Some(leadDays) =&gt; {
            try {
            leadDays.toInt
            } catch {
                case e: Exception =&gt; throw new Exception(s&quot;cannot convert leadDays ('${leadDays}') to integer.&quot;)
            }
        }
        case None =&gt; throw new Exception(&quot;required parameter 'leadDays' not found.&quot;)
    }

    val formatter = params.get(&quot;pattern&quot;) match {
        case Some(pattern) =&gt; {
            try {
                DateTimeFormatter.ofPattern(pattern).withResolverStyle(ResolverStyle.SMART)
            } catch {
                case e: Exception =&gt; throw new Exception(s&quot;cannot parse pattern ('${pattern}').&quot;)
            }
        }
        case None =&gt; throw new Exception(&quot;required parameter 'pattern' not found.&quot;)
    }

    val currentDate = params.get(&quot;currentDate&quot;) match {
        case Some(currentDate) =&gt; {
            try {
                LocalDate.parse(currentDate, formatter)
            } catch {
                case e: Exception =&gt; throw new Exception(s&quot;&quot;&quot;cannot parse currentDate ('${currentDate}') with formatter '${params.get(&quot;pattern&quot;).getOrElse(&quot;&quot;)}'.&quot;&quot;&quot;)
            }
        }
        case None =&gt; java.time.LocalDate.now
    }


    // calculate the range 
    // produces a value that looks like &quot;2018-12-31,2019-01-01,2019-01-02,2019-01-03,2019-01-04,2019-01-05,2019-01-06&quot;
    val res = (lagDays to leadDays).map { v =&gt;
      formatter.format(currentDate.plusDays(v))
    }.mkString(&quot;,&quot;)

    // set the return value
    val values = new java.util.HashMap[String, Object]()
    values.put(returnName, res)

    logger.info()
      .field(&quot;event&quot;, &quot;exit&quot;)
      .field(&quot;duration&quot;, System.currentTimeMillis() - startTime)
      .map(&quot;stage&quot;, stageDetail)      
      .log()  

    values
  }
}
</code></pre>

<p>The plugin then needs to be registered in the <code>plugins.config</code> section of the job configuration and the full plugin name must be listed in your project&rsquo;s <code>/resources/META-INF/services/ai.tripl.arc.plugins.DynamicConfigurationPlugin</code> file. See <a href="https://github.com/tripl-ai/arc/blob/master/src/test/resources/META-INF/services/ai.tripl.arc.plugins.DynamicConfigurationPlugin">this example</a>.</p>

<p>Note that the resolution order of these plugins is in descending order in that if the the <code>ETL_CONF_LAST_PROCESSING_DAY</code> was declared in multiple plugins the value set by the plugin with the lower index in the <code>plugins.config</code> array will take precedence.</p>

<p>The <code>ETL_CONF_LAST_PROCESSING_DAY</code> variable is then available to be resolved in a standard configuration:</p>

<pre><code class="language-json">{
  &quot;plugins&quot;: {
    &quot;config&quot;: [
      {
        &quot;type&quot;: &quot;ai.tripl.arc.plugins.config.DeltaPeriodDynamicConfigurationPlugin&quot;,
        &quot;environments&quot;: [
          &quot;production&quot;,
          &quot;test&quot;
        ],
        &quot;params&quot;: {
          &quot;returnName&quot;: &quot;ETL_CONF_DELTA_PERIOD&quot;,
          &quot;lagDays&quot;: &quot;10&quot;,
          &quot;leadDays&quot;: &quot;1&quot;,
          &quot;pattern&quot;: &quot;yyyy-MM-dd&quot;
        }
      }
    ]
  },
  &quot;stages&quot;: [
    {
      &quot;type&quot;: &quot;DelimitedExtract&quot;,
      &quot;name&quot;: &quot;load customer extract&quot;,
      &quot;environments&quot;: [
        &quot;production&quot;,
        &quot;test&quot;
      ],
      &quot;inputURI&quot;: &quot;hdfs://datalake/input/customer/customers_{&quot;${ETL_CONF_DELTA_PERIOD}&quot;}.csv&quot;,
      &quot;outputView&quot;: &quot;customer&quot;
    }
  ]
}
</code></pre>

<h2 id="lifecycle-plugins">Lifecycle Plugins</h2>

<h5 id="since-1-3-0-1">Since: 1.3.0</h5>

<p>Custom <code>Lifecycle Plugins</code> allow users to extend the base Arc framework with logic which is executed <code>before</code> or <code>after</code> each Arc stage (lifecycle hooks). These stages are useful for implementing things like dataset logging after each stage execution for debugging.</p>

<h3 id="examples-1">Examples</h3>

<pre><code class="language-scala">package ai.tripl.arc.plugins.lifecycle

import java.util

import org.apache.spark.sql.{DataFrame, SparkSession}

import ai.tripl.arc.api.API._
import ai.tripl.arc.plugins.LifecyclePlugin
import ai.tripl.arc.util.Utils
import ai.tripl.arc.util.log.logger.Logger

class DataFramePrinterLifecyclePlugin extends LifecyclePlugin {

  var params = Map[String, String]()

  override def setParams(p: Map[String, String]) {
    params = p
  }

  override def before(stage: PipelineStage)(implicit spark: SparkSession, logger: ai.tripl.arc.util.log.logger.Logger) {
    logger.trace()        
      .field(&quot;event&quot;, &quot;before&quot;)
      .field(&quot;stage&quot;, stage.name)
      .field(&quot;stageType&quot;, stage.getType)
      .log()  
  }

  override def after(stage: PipelineStage, result: Option[DataFrame], isLast: Boolean)(implicit spark: SparkSession, logger: ai.tripl.arc.util.log.logger.Logger) {
    logger.trace()        
      .field(&quot;event&quot;, &quot;after&quot;)
      .field(&quot;stage&quot;, stage.name)
      .field(&quot;stageType&quot;, stage.getType)
      .field(&quot;isLast&quot;, java.lang.Boolean.valueOf(isLast))
      .log() 

    result match {
      case Some(df) =&gt; {
        val numRows = params.get(&quot;numRows&quot;) match {
          case Some(n) =&gt; n.toInt
          case None =&gt; 20
        }

        val truncate = params.get(&quot;truncate&quot;) match {
          case Some(t) =&gt; t.toBoolean
          case None =&gt; true
        }  

        df.show(numRows, truncate)
      }
      case None =&gt;
    }
  }

}
</code></pre>

<p>The plugin then needs to be registered by adding the full plugin name must be listed in your project’s <code>/resources/META-INF/services/ai.tripl.arc.plugins.LifecyclePlugin</code> file.</p>

<p>To execute:</p>

<pre><code class="language-json">{
  &quot;plugins&quot;: {
    &quot;lifecycle&quot;: [
      {
        &quot;type&quot;: &quot;ai.tripl.arc.plugins.lifecycle.DataFramePrinterLifecyclePlugin&quot;,
        &quot;environments&quot;: [
          &quot;production&quot;,
          &quot;test&quot;
        ],
        &quot;params&quot;: {
          &quot;numRows&quot;: &quot;100&quot;,
          &quot;truncate&quot;: &quot;false&quot;,
        }
      }
    ]
  },
  &quot;stages&quot;: [
    ...
  ]
}
</code></pre>

<h2 id="pipeline-stage-plugins">Pipeline Stage Plugins</h2>

<h5 id="since-1-3-0-2">Since: 1.3.0</h5>

<p>Custom <code>Pipeline Stage Plugins</code> allow users to extend the base Arc framework with custom stages which allow the full use of the Spark <a href="https://spark.apache.org/docs/latest/api/scala/">Scala API</a>. This means that private business logic or code which relies on libraries not included in the base Arc framework can be used - however it is strongly advised to use the inbuilt SQL stages where possible. These stages can use the <code>params</code> map to be able to pass configuration parameters.</p>

<p>If stages are general purpose enough for use outside your organisation consider creating a pull request against the main <a href="https://github.com/tripl-ai/arc">Arc repository</a> so that others can benefit.</p>

<h3 id="examples-2">Examples</h3>

<pre><code class="language-scala">package au.com.myfakebusiness.plugins

import ai.tripl.arc.plugins
import ai.tripl.arc.util.log.logger.Logger
import org.apache.spark.sql.{DataFrame, SparkSession}

class MyFakeBusinessAddCopyrightStage extends PipelineStagePlugin {
  override def execute(name: String, params: Map[String, String])(implicit spark: SparkSession, logger: Logger): Option[DataFrame] = {
    val startTime = System.currentTimeMillis() 

    val inputView = params.get(&quot;inputView&quot;).getOrElse(&quot;&quot;)
    val outputView = params.get(&quot;outputView&quot;).getOrElse(&quot;&quot;)
    val copyrightStatement = params.get(&quot;copyrightStatement&quot;).getOrElse(&quot;&quot;)

    val stageDetail = new java.util.HashMap[String, Object]()
    stageDetail.put(&quot;name&quot;, name)
    stageDetail.put(&quot;inputView&quot;, inputView)  
    stageDetail.put(&quot;outputView&quot;, outputView)  
    stageDetail.put(&quot;copyrightStatement&quot;, copyrightStatement)
    
    logger.info()
      .field(&quot;event&quot;, &quot;enter&quot;)
      .map(&quot;stage&quot;, stageDetail)      
      .log()


    // get existing dataframe
    val df = spark.table(inputView)

    // add copyright statement
    val enrichedDF = df.withColumn(&quot;copyright&quot;, lit(copyrightStatement))

    // register output view
    enrichedDF.createOrReplaceTempView(outputView)

    logger.info()
      .field(&quot;event&quot;, &quot;exit&quot;)
      .field(&quot;duration&quot;, System.currentTimeMillis() - startTime)
      .map(&quot;stage&quot;, stageDetail)      
      .log() 

    Option(enrichedDF)    
  }
}
</code></pre>

<p>The plugin then needs to be registered by adding the full plugin name must be listed in your project’s <code>/resources/META-INF/services/ai.tripl.arc.plugins.PipelineStagePlugin</code> file.</p>

<p>To execute:</p>

<pre><code class="language-json">{
  &quot;stages&quot;: [
    {
      &quot;type&quot;: &quot;au.com.mybusiness.plugins.MyFakeBusinessAddCopyrightStage&quot;,
      &quot;name&quot;: &quot;add copyright to each row&quot;,
      &quot;environments&quot;: [
        &quot;production&quot;,
        &quot;test&quot;
      ],
      &quot;params&quot;: {
        &quot;inputView&quot;: &quot;calculated_dataset&quot;,
        &quot;copyrightStatement&quot;: &quot;copyright 2018 MyBusiness.com.au&quot;,
        &quot;outputView&quot;: &quot;final_dataset&quot;
      }
    }
  ]
}
</code></pre>

<h2 id="user-defined-functions">User Defined Functions</h2>

<h5 id="since-1-3-0-3">Since: 1.3.0</h5>

<div class="admonition note">
<p class="admonition-title">User Defined Functions vs Spark SQL Functions</p>
<p>The inbuilt <a href="https://spark.apache.org/docs/latest/api/sql/index.html">Spark SQL Functions</a> are heavily optimised by the internal Spark code to a level which custom User Defined Functions cannot be (byte code) - so where possible it is better to use the inbuilt functions.</p>
</div>

<p><code>User Defined Functions</code> allow users to extend the <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> dialect.</p>

<p>Arc already includes <a href="partials/#user-defined-functions">some addtional functions</a> which are not included in the base <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> dialect so any useful generic functions can be included in the <a href="https://github.com/tripl-ai/arc">Arc repository</a> so that others can benefit.</p>

<h3 id="examples-3">Examples</h3>

<p>Write the code to define the custom <code>User Defined Function</code>:</p>

<pre><code class="language-scala">package ai.tripl.arc.plugins
import java.util

import org.apache.spark.sql.SQLContext

import ai.tripl.arc.util.log.logger.Logger

class UDFPluginTest extends UDFPlugin {
  // one udf plugin can register multiple user defined functions
  override def register(sqlContext: SQLContext)(implicit logger: ai.tripl.arc.util.log.logger.Logger): Seq[String] = {
    
    // register the functions so they can be accessed via Spark SQL
    // SELECT add_ten(1) AS one_plus_ten
    sqlContext.udf.register(&quot;add_ten&quot;, UDFPluginTest.addTen _ )
    
    // return the list of udf names that were registered for logging
    Seq(&quot;add_ten&quot;)
  }
}

object UDFPluginTest {
  // add 10 to an incoming integer - DO NOT DO THIS IN PRODUCTION INSTEAD USE SPARK SQL DIRECTLY
  def addTen(input: Int): Int = {
    input + 10
  }
}
</code></pre>

<p>The plugin then needs to be registered by adding the full plugin name must be listed in your project&rsquo;s <code>/resources/META-INF/services/ai.tripl.arc.plugins.UDFPlugin</code> file.</p>


			<aside class="copyright" role="note">
				
				&copy; 2019 Released under the MIT license
				
			</aside>

			<footer class="footer">
				

<nav class="pagination" aria-label="Footer">
  <div class="previous">
    
    <a href="https://arc.tripl.ai/deploy/" title="Deploy">
      <span class="direction">
        Previous
      </span>
      <div class="page">
        <div class="button button-previous" role="button" aria-label="Previous">
          <i class="icon icon-back"></i>
        </div>
        <div class="stretch">
          <div class="title">
            Deploy
          </div>
        </div>
      </div>
    </a>
    
  </div>

  <div class="next">
    
    <a href="https://arc.tripl.ai/contributing/" title="Contributing">
      <span class="direction">
        Next
      </span>
      <div class="page">
        <div class="stretch">
          <div class="title">
            Contributing
          </div>
        </div>
        <div class="button button-next" role="button" aria-label="Next">
          <i class="icon icon-forward"></i>
        </div>
      </div>
    </a>
    
  </div>
</nav>




			</footer>
		</div>
	</article>

	<div class="results" role="status" aria-live="polite">
		<div class="scrollable">
			<div class="wrapper">
				<div class="meta"></div>
				<div class="list"></div>
			</div>
		</div>
	</div>
</main>

    <script>
    
      var base_url = 'https:\/\/arc.tripl.ai\/';
      var repo_id  = 'tripl-ai\/arc';
    
    </script>

    <script src="https://arc.tripl.ai/javascripts/application.js"></script>
    

    <script>
      /* Add headers to scrollspy */
      var headers   = document.getElementsByTagName("h2");
      var scrollspy = document.getElementById('scrollspy');

      if(scrollspy) {
        if(headers.length > 0) {
          for(var i = 0; i < headers.length; i++) {
            var li = document.createElement("li");
            li.setAttribute("class", "anchor");

            var a  = document.createElement("a");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", headers[i].innerHTML);
            a.innerHTML = headers[i].innerHTML;

            li.appendChild(a)
            scrollspy.appendChild(li);
          }
        } else {
          scrollspy.parentElement.removeChild(scrollspy)
        }


        /* Add permanent link next to the headers */
        var headers = document.querySelectorAll("h1, h2, h3, h4, h5, h6");

        for(var i = 0; i < headers.length; i++) {
            var a = document.createElement("a");
            a.setAttribute("class", "headerlink");
            a.setAttribute("href", "#" + headers[i].id);
            a.setAttribute("title", "Permanent link")
            a.innerHTML = "#";
            headers[i].appendChild(a);
        }
      }
    </script>

    

    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/highlight.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.8.0/languages/scala.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

